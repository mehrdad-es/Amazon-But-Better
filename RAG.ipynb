{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import cohere\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.llms import Cohere\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage,HumanMessage\n",
    "# os.environ['COHERE_API_KEY']="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "# from langchain.llms import Cohere\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm=Cohere()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir('./oldBios/')):\n",
    "    azBookSummary=open(f'./oldBios/{file}','r').read()\n",
    "    person=llm_chain.run(f\"who is this biography ({azBookSummary})about? just give me the answer, no\\\n",
    "                  no explanations\")\n",
    "    print(person)\n",
    "    wikiPersonDescription=llm_chain.run(f'give me wikipedia summary of {person}\\\n",
    "                                    in 100 words.')\n",
    "    print(wikiPersonDescription)\n",
    "    with open(f'./augBios/{file}','w') as f:\n",
    "        f.write(wikiPersonDescription.replace('\\n','')+'///'+azBookSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "path='./oldBios/'\n",
    "# path='./augBios/'\n",
    "for file in os.listdir(path):\n",
    "    loader = TextLoader(f'{path}{file}',encoding='unicode_escape')\n",
    "    # loader.load()[0].metadata['category']='biography'\n",
    "    # print(loader.load()[0].metadata)\n",
    "    documents += loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = CohereEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=Cohere(), chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={'k':1}),return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = CohereEmbeddings(model='embed-english-v3.0')\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "cohereLLM=Cohere(model='command')\n",
    "memory=ConversationSummaryMemory(\n",
    "    llm=cohereLLM, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "# qa = ConversationalRetrievalChain.from_llm(llm=Cohere(model='command'), \\\n",
    "#     retriever=docsearch.as_retriever(),return_source_documents=True,memory=memory)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=Cohere(model='command'), \\\n",
    "    retriever=docsearch.as_retriever(),memory=memory)\n",
    "question_generator = LLMChain(llm=cohereLLM, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "doc_chain = load_qa_with_sources_chain(cohereLLM, chain_type=\"refine\")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True\n",
    ")\n",
    "# print(qa('hello'))\n",
    "# print(chain({'question':'hello','chat_history':[]}))\n",
    "def predict(message, history):\n",
    "    # history_langchain_format = []\n",
    "    # for human, ai in history:\n",
    "    #     history_langchain_format.append(HumanMessage(content=human))\n",
    "    #     history_langchain_format.append(AIMessage(content=ai))\n",
    "    # history_langchain_format.append(HumanMessage(content=message))\n",
    "    # gpt_response = llm(history_langchain_format)\n",
    "    # return gpt_response.content\n",
    "    \n",
    "    # message=message+'? just the book title-Autho'\n",
    "    #---------\n",
    "    # for human, ai in history:\n",
    "    #     history_langchain_format.append(HumanMessage(content=human))\n",
    "    #     history_langchain_format.append(AIMessage(content=ai))\n",
    "    # history_langchain_format.append(HumanMessage(content=message))\n",
    "    # gpt_response = llm(history_langchain_format)\n",
    "    # return gpt_response.content\n",
    "    #------------\n",
    "    message=\"you are a language model that gives book recommendation\"+message+\\\n",
    "    'just give the book title and author'\n",
    "        # chat_history=[]\n",
    "        # result = chain({'question':message,'chat_history':chat_history})\n",
    "    result=qa(message)\n",
    "    # r1=docsearch.similarity_search_with_score(query=q,k=3)\n",
    "    # print([(item[-2].metadata,item[-1]) for item in r1],\\\n",
    "    #       '\\n\\n',result['result'],f'|| {result[\"source_documents\"][0].metadata}','\\n*****\\n')\n",
    "    # if result['result'] not in [\"I don't know\",\"I don't know.\"]:\n",
    "    #     return result['result']+f'\\n---\\nAmazon Kindle ebook description is:\\n {result[\"source_documents\"][0].page_content}'+\\\n",
    "    #     f'\\nfrom this file: {result[\"source_documents\"][0].metadata}'\n",
    "    # else:\n",
    "        # return result['result'] \n",
    "    # print(result)\n",
    "    return result['answer']   \n",
    "'''------'''\n",
    "gr.ChatInterface(predict,\n",
    "    chatbot=gr.Chatbot(height='auto'),\n",
    "    textbox=gr.Textbox(placeholder=\"Recommend a book on someone who...\"),\n",
    "    title=\"Amazon But Better\",\n",
    "    description=\"Amazon started out with selling books. However, searching books on \\\n",
    "    Amazon is tedious and inaccurate if you don't know what you are exactly looking for. **Why not \\\n",
    "    make it faster and easier with LLMs:).**\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = CohereEmbeddings(model='embed-english-v3.0')\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=Cohere(model='command'), chain_type=\"stuff\", \\\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k':1}),return_source_documents=True)\n",
    "# j=[HumanMessage(content='hello')]\n",
    "# j.append(AIMessage(content='hi'))\n",
    "# print(j)\n",
    "# print(qa({'query': ''.join([HumanMessage(content='hello')].append(AIMessage(content='hi')))}))\n",
    "history=[]\n",
    "def predict(message, history):\n",
    "    # gpt_response = qa({'query':''.join(history)+f'.\\n given the previous conversation respond using the following prompt:{message}'})\n",
    "    # # print(gpt_response)\n",
    "    # history.append((f'HumanMessage:{message}',f'AIMessage: {gpt_response},'))\n",
    "    # # history=history_langchain_format\n",
    "    # return gpt_response['result']\n",
    "    \n",
    "    # message=message+'? just the book title-Author'\n",
    "    # result = qa({\"query\": message})\n",
    "    # # r1=docsearch.similarity_search_with_score(query=q,k=3)\n",
    "    # # print([(item[-2].metadata,item[-1]) for item in r1],\\\n",
    "    # #       '\\n\\n',result['result'],f'|| {result[\"source_documents\"][0].metadata}','\\n*****\\n')\n",
    "    # if result['result'] not in [\"I don't know\",\"I don't know.\"]:\n",
    "    #     return result['result']+f'\\n---\\nAmazon Kindle ebook description is:\\n {result[\"source_documents\"][0].page_content}'+\\\n",
    "    #     f'\\nfrom this file: {result[\"source_documents\"][0].metadata}'\n",
    "    # else:\n",
    "    #     return result['result']\n",
    "\n",
    "gr.ChatInterface(predict,\n",
    "    chatbot=gr.Chatbot(height='auto'),\n",
    "    textbox=gr.Textbox(placeholder=\"Recommend a book on someone who...\"),\n",
    "    title=\"Amazon But Better\",\n",
    "    description=\"Amazon started out with selling books. However, searching books on \\\n",
    "    Amazon is tedious and inaccurate if you don't know what you are exactly looking for. **Why not \\\n",
    "    make it faster and easier with LLMs:).**\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs=[\n",
    "    'recommend a books on someone who was born in the state of mississpi?',\n",
    "    'recommend a books on someone who has won mr.olympia?',\n",
    "    'recommend a books on someone who started body building at 20?',\n",
    "    'recommend a books on someone who played in magic mike?',\n",
    "    'recommend a books on someone who was actor born in the uk?'\n",
    "]\n",
    "for q in qs:\n",
    "    query = f\"{q} Just give me the book title-Author\"\n",
    "    result = qa({\"query\": query})\n",
    "    r1=docsearch.similarity_search_with_score(query=q,k=3)\n",
    "    print([(item[-2].metadata,item[-1]) for item in r1],\\\n",
    "          '\\n\\n',result['result'],f'|| {result[\"source_documents\"][0].metadata}','\\n*****\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
